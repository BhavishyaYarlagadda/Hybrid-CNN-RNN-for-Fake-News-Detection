{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import nltk\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import PorterStemmer\n",
    "import re\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import LabelEncoder, OneHotEncoder\n",
    "from zipfile import ZipFile\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the FA-KES dataset\n",
    "df_fakes = pd.read_csv(\"FA-KES-Dataset.csv\", encoding='ISO-8859-1')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dataset Splitting for FA-KES dataset\n",
    "#train_test_split fcn scikit-learn library to split  into training and testing.\n",
    "X_fakes = df_fakes[\"article_title\"].values\n",
    "y_fakes = df_fakes[\"labels\"].values\n",
    "\n",
    "X_fakes_train, X_fakes_test, y_fakes_train, y_fakes_test = train_test_split(X_fakes, y_fakes, test_size=0.2, random_state=42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "No stemming"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\bhavi\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\bhavi\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "import re\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "\n",
    "nltk.download('punkt')\n",
    "nltk.download('stopwords')\n",
    "\n",
    "def text_preprocessing(text):\n",
    "    text = re.sub(r'\\b(?:\\d{1,3}\\.){3}\\d{1,3}\\b', '', text)  # Regular expression to remove any IP addresses\n",
    "    text = re.sub(r'http\\S+|www\\S+|https\\S+', '', text)  # Remove URLs\n",
    "    tokens = word_tokenize(text.lower())\n",
    "    stop_words = set(stopwords.words('english'))\n",
    "    tokens = [token for token in tokens if token not in stop_words]\n",
    "    preprocessed_text = ' '.join(tokens)\n",
    "    return preprocessed_text\n",
    "\n",
    "X_fakes_train = [text_preprocessing(text) for text in X_fakes_train]\n",
    "X_fakes_test = [text_preprocessing(text) for text in X_fakes_test]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "import zipfile\n",
    "import os\n",
    "\n",
    "# Define the URL of the GloVe embeddings\n",
    "glove_url = \"http://nlp.stanford.edu/data/glove.6B.zip\"\n",
    "\n",
    "# Define the local file name to save the downloaded embeddings\n",
    "local_filename = \"glove.6B.zip\"\n",
    "\n",
    "# Download the GloVe embeddings file\n",
    "response = requests.get(glove_url, stream=True)\n",
    "with open(local_filename, 'wb') as file:\n",
    "    for chunk in response.iter_content(chunk_size=1024):\n",
    "        if chunk:\n",
    "            file.write(chunk)\n",
    "\n",
    "# Unzip the downloaded file\n",
    "with zipfile.ZipFile(local_filename, 'r') as zip_ref:\n",
    "    zip_ref.extractall()\n",
    "\n",
    "# Remove the zip file if needed\n",
    "os.remove(local_filename)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 400000 word vectors.\n"
     ]
    }
   ],
   "source": [
    "from keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "import os\n",
    "import numpy as np\n",
    "\n",
    "# Load the pre-trained GloVe word embeddings\n",
    "embedding_dim = 100\n",
    "glove_file = \"glove.6B.100d.txt\"\n",
    "\n",
    "embeddings_index = {}\n",
    "with open(glove_file, 'r', encoding='utf-8') as f:\n",
    "    for line in f:\n",
    "        values = line.split()\n",
    "        word = values[0]\n",
    "        coefs = np.asarray(values[1:], dtype='float32')\n",
    "        embeddings_index[word] = coefs\n",
    "\n",
    "print('Found %s word vectors.' % len(embeddings_index))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create tokenizer for FA-KES dataset\n",
    "tokenizer_fakes = Tokenizer()\n",
    "tokenizer_fakes.fit_on_texts(X_fakes_train)\n",
    "\n",
    "# Convert text to sequences of integers and apply post-padding\n",
    "X_fakes_train_sequences = tokenizer_fakes.texts_to_sequences(X_fakes_train)\n",
    "X_fakes_test_sequences = tokenizer_fakes.texts_to_sequences(X_fakes_test)\n",
    "\n",
    "max_sequence_length = 300\n",
    "X_fakes_train_padded = pad_sequences(X_fakes_train_sequences, maxlen=max_sequence_length)\n",
    "X_fakes_test_padded = pad_sequences(X_fakes_test_sequences, maxlen=max_sequence_length)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prepare embedding matrix for FA-KES dataset\n",
    "word_index_fakes = tokenizer_fakes.word_index\n",
    "num_words_fakes = min(len(word_index_fakes), len(embeddings_index)) + 1\n",
    "embedding_matrix_fakes = np.zeros((num_words_fakes, embedding_dim))\n",
    "\n",
    "for word, i in word_index_fakes.items():\n",
    "    if i >= num_words_fakes:\n",
    "        continue\n",
    "    embedding_vector = embeddings_index.get(word)\n",
    "    if embedding_vector is not None:\n",
    "        embedding_matrix_fakes[i] = embedding_vector"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Hybrid CNN-RNN(LSTM)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "9/9 [==============================] - 3s 155ms/step - loss: 0.7062 - accuracy: 0.4475 - val_loss: 0.6918 - val_accuracy: 0.5271\n",
      "Epoch 2/10\n",
      "9/9 [==============================] - 1s 90ms/step - loss: 0.6817 - accuracy: 0.5467 - val_loss: 0.7105 - val_accuracy: 0.5116\n",
      "Epoch 3/10\n",
      "9/9 [==============================] - 1s 91ms/step - loss: 0.6594 - accuracy: 0.5584 - val_loss: 0.6877 - val_accuracy: 0.5194\n",
      "Epoch 4/10\n",
      "9/9 [==============================] - 1s 92ms/step - loss: 0.6653 - accuracy: 0.5623 - val_loss: 0.6906 - val_accuracy: 0.4729\n",
      "Epoch 5/10\n",
      "9/9 [==============================] - 1s 94ms/step - loss: 0.6254 - accuracy: 0.7276 - val_loss: 0.6891 - val_accuracy: 0.5349\n",
      "Epoch 6/10\n",
      "9/9 [==============================] - 1s 96ms/step - loss: 0.5953 - accuracy: 0.6965 - val_loss: 0.6776 - val_accuracy: 0.5504\n",
      "Epoch 7/10\n",
      "9/9 [==============================] - 1s 96ms/step - loss: 0.5760 - accuracy: 0.7626 - val_loss: 0.7093 - val_accuracy: 0.5271\n",
      "Epoch 8/10\n",
      "9/9 [==============================] - 1s 98ms/step - loss: 0.5148 - accuracy: 0.8035 - val_loss: 0.8251 - val_accuracy: 0.4884\n",
      "Epoch 9/10\n",
      "9/9 [==============================] - 1s 96ms/step - loss: 0.5214 - accuracy: 0.7179 - val_loss: 0.7534 - val_accuracy: 0.5271\n",
      "Epoch 10/10\n",
      "9/9 [==============================] - 1s 100ms/step - loss: 0.4876 - accuracy: 0.7665 - val_loss: 0.7178 - val_accuracy: 0.5504\n"
     ]
    }
   ],
   "source": [
    "from keras.models import Sequential\n",
    "from keras.layers import Embedding, Conv1D, MaxPooling1D, LSTM, Dense\n",
    "\n",
    "# Model Architecture\n",
    "def create_model(embedding_matrix, max_sequence_length, embedding_dim):\n",
    "    model = Sequential()\n",
    "    model.add(Embedding(embedding_matrix.shape[0], embedding_dim, weights=[embedding_matrix], input_length=max_sequence_length, trainable=False))\n",
    "    model.add(Conv1D(filters=128, kernel_size=5, activation='relu'))\n",
    "    model.add(MaxPooling1D(pool_size=2))\n",
    "    model.add(LSTM(32))\n",
    "    model.add(Dense(1, activation='sigmoid'))\n",
    "\n",
    "    # Compile the model\n",
    "    model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])\n",
    "    return model\n",
    "# Create the model for FA-KES dataset\n",
    "model_fakes = create_model(embedding_matrix_fakes, max_sequence_length, embedding_dim)\n",
    "\n",
    "history_fakes = model_fakes.fit(X_fakes_train_padded, y_fakes_train, epochs=10, batch_size=64, validation_split=0.2, verbose=1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6/6 [==============================] - 0s 20ms/step\n"
     ]
    }
   ],
   "source": [
    "  # Evaluate the models on the test sets\n",
    "y_fakes_pred_probs = model_fakes.predict(X_fakes_test_padded)\n",
    "y_fakes_pred = np.argmax(y_fakes_pred_probs, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.515527950310559\n",
      "Precision: 0.5523809523809524\n",
      "Recall: 0.651685393258427\n",
      "F1 Score: 0.597938144329897\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\n",
    "\n",
    "# Convert predicted probabilities to binary predictions (0 or 1)\n",
    "y_fakes_pred_binary = (y_fakes_pred_probs > 0.5).astype(int)\n",
    "\n",
    "# Calculate accuracy\n",
    "accuracy_fakes = accuracy_score(y_fakes_test, y_fakes_pred_binary)\n",
    "\n",
    "# Calculate precision, recall, and F1 score\n",
    "precision_fakes = precision_score(y_fakes_test, y_fakes_pred_binary)\n",
    "recall_fakes = recall_score(y_fakes_test, y_fakes_pred_binary)\n",
    "f1_score_fakes = f1_score(y_fakes_test, y_fakes_pred_binary)\n",
    "\n",
    "print(\"Accuracy:\", accuracy_fakes)\n",
    "print(\"Precision:\", precision_fakes)\n",
    "print(\"Recall:\", recall_fakes)\n",
    "print(\"F1 Score:\", f1_score_fakes)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Hybrid CNN-RNN(BiLSTM)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.layers import Embedding, Conv1D, MaxPooling1D, Bidirectional, LSTM, Dense\n",
    "\n",
    "#Hybrid CNN-RNN BiLSTM\n",
    "\n",
    "def create_model(embedding_matrix, max_sequence_length, embedding_dim):\n",
    "    model = Sequential()\n",
    "    model.add(Embedding(embedding_matrix.shape[0], embedding_dim, weights=[embedding_matrix], input_length=max_sequence_length, trainable=False))\n",
    "    model.add(Conv1D(filters=128, kernel_size=5, activation='relu'))\n",
    "    model.add(MaxPooling1D(pool_size=2))\n",
    "    model.add(Bidirectional(LSTM(32)))  # Use Bidirectional LSTM\n",
    "    model.add(Dense(1, activation='sigmoid'))\n",
    "\n",
    "    # Compile the model\n",
    "    model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "9/9 [==============================] - 6s 239ms/step - loss: 0.6979 - accuracy: 0.5058 - val_loss: 0.6924 - val_accuracy: 0.5039\n",
      "Epoch 2/10\n",
      "9/9 [==============================] - 2s 246ms/step - loss: 0.6716 - accuracy: 0.6323 - val_loss: 0.6841 - val_accuracy: 0.5581\n",
      "Epoch 3/10\n",
      "9/9 [==============================] - 2s 249ms/step - loss: 0.6608 - accuracy: 0.6401 - val_loss: 0.6859 - val_accuracy: 0.5736\n",
      "Epoch 4/10\n",
      "9/9 [==============================] - 2s 243ms/step - loss: 0.6408 - accuracy: 0.6498 - val_loss: 0.6856 - val_accuracy: 0.5736\n",
      "Epoch 5/10\n",
      "9/9 [==============================] - 2s 232ms/step - loss: 0.6465 - accuracy: 0.6187 - val_loss: 0.7011 - val_accuracy: 0.5271\n",
      "Epoch 6/10\n",
      "9/9 [==============================] - 2s 218ms/step - loss: 0.6645 - accuracy: 0.5428 - val_loss: 0.6941 - val_accuracy: 0.5039\n",
      "Epoch 7/10\n",
      "9/9 [==============================] - 2s 227ms/step - loss: 0.6284 - accuracy: 0.6712 - val_loss: 0.6840 - val_accuracy: 0.5504\n",
      "Epoch 8/10\n",
      "9/9 [==============================] - 2s 213ms/step - loss: 0.6134 - accuracy: 0.6946 - val_loss: 0.6849 - val_accuracy: 0.5891\n",
      "Epoch 9/10\n",
      "9/9 [==============================] - 2s 207ms/step - loss: 0.5745 - accuracy: 0.7549 - val_loss: 0.6848 - val_accuracy: 0.5969\n",
      "Epoch 10/10\n",
      "9/9 [==============================] - 2s 239ms/step - loss: 0.5316 - accuracy: 0.7821 - val_loss: 0.6966 - val_accuracy: 0.5581\n",
      "6/6 [==============================] - 1s 29ms/step\n",
      "Accuracy: 0.453416149068323\n",
      "Precision: 0.5061728395061729\n",
      "Recall: 0.4606741573033708\n",
      "F1 Score: 0.4823529411764706\n"
     ]
    }
   ],
   "source": [
    "# Create the model for FA-KES dataset with Bidirectional LSTM\n",
    "model_fakes = create_model(embedding_matrix_fakes, max_sequence_length, embedding_dim)\n",
    "\n",
    "# Train the model\n",
    "history_fakes = model_fakes.fit(X_fakes_train_padded, y_fakes_train, epochs=10, batch_size=64, validation_split=0.2, verbose=1)\n",
    "\n",
    "# Evaluate the model on the test set\n",
    "y_fakes_pred_probs = model_fakes.predict(X_fakes_test_padded)\n",
    "y_fakes_pred = (y_fakes_pred_probs > 0.5).astype(int)  # Convert predicted probabilities to binary predictions (0 or 1)\n",
    "\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\n",
    "\n",
    "# Calculate accuracy\n",
    "accuracy_hcnnrnn = accuracy_score(y_fakes_test, y_fakes_pred)\n",
    "\n",
    "# Calculate precision, recall, and F1 score\n",
    "precision_hcnnrnn = precision_score(y_fakes_test, y_fakes_pred)\n",
    "recall_hcnnrnn = recall_score(y_fakes_test, y_fakes_pred)\n",
    "f1_hcnnrnn = f1_score(y_fakes_test, y_fakes_pred)\n",
    "\n",
    "print(\"Accuracy:\", accuracy_hcnnrnn)\n",
    "print(\"Precision:\", precision_hcnnrnn)\n",
    "print(\"Recall:\", recall_hcnnrnn)\n",
    "print(\"F1 Score:\", f1_hcnnrnn)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Transformer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: transformers in c:\\users\\bhavi\\anaconda3\\envs\\myenv\\lib\\site-packages (4.30.2)\n",
      "Requirement already satisfied: tqdm>=4.27 in c:\\users\\bhavi\\anaconda3\\envs\\myenv\\lib\\site-packages (from transformers) (4.66.1)\n",
      "Requirement already satisfied: safetensors>=0.3.1 in c:\\users\\bhavi\\anaconda3\\envs\\myenv\\lib\\site-packages (from transformers) (0.3.3)\n",
      "Requirement already satisfied: tokenizers!=0.11.3,<0.14,>=0.11.1 in c:\\users\\bhavi\\anaconda3\\envs\\myenv\\lib\\site-packages (from transformers) (0.13.3)\n",
      "Requirement already satisfied: filelock in c:\\users\\bhavi\\anaconda3\\envs\\myenv\\lib\\site-packages (from transformers) (3.12.2)\n",
      "Requirement already satisfied: importlib-metadata in c:\\users\\bhavi\\anaconda3\\envs\\myenv\\lib\\site-packages (from transformers) (6.7.0)\n",
      "Requirement already satisfied: huggingface-hub<1.0,>=0.14.1 in c:\\users\\bhavi\\anaconda3\\envs\\myenv\\lib\\site-packages (from transformers) (0.16.4)\n",
      "Requirement already satisfied: pyyaml>=5.1 in c:\\users\\bhavi\\anaconda3\\envs\\myenv\\lib\\site-packages (from transformers) (6.0.1)\n",
      "Requirement already satisfied: numpy>=1.17 in c:\\users\\bhavi\\anaconda3\\envs\\myenv\\lib\\site-packages (from transformers) (1.21.5)\n",
      "Requirement already satisfied: requests in c:\\users\\bhavi\\anaconda3\\envs\\myenv\\lib\\site-packages (from transformers) (2.28.1)\n",
      "Requirement already satisfied: packaging>=20.0 in c:\\users\\bhavi\\anaconda3\\envs\\myenv\\lib\\site-packages (from transformers) (23.2)\n",
      "Requirement already satisfied: regex!=2019.12.17 in c:\\users\\bhavi\\anaconda3\\envs\\myenv\\lib\\site-packages (from transformers) (2023.10.3)\n",
      "Requirement already satisfied: fsspec in c:\\users\\bhavi\\anaconda3\\envs\\myenv\\lib\\site-packages (from huggingface-hub<1.0,>=0.14.1->transformers) (2023.1.0)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in c:\\users\\bhavi\\anaconda3\\envs\\myenv\\lib\\site-packages (from huggingface-hub<1.0,>=0.14.1->transformers) (4.3.0)\n",
      "Requirement already satisfied: colorama in c:\\users\\bhavi\\anaconda3\\envs\\myenv\\lib\\site-packages (from tqdm>=4.27->transformers) (0.4.6)\n",
      "Requirement already satisfied: zipp>=0.5 in c:\\users\\bhavi\\anaconda3\\envs\\myenv\\lib\\site-packages (from importlib-metadata->transformers) (3.15.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\bhavi\\anaconda3\\envs\\myenv\\lib\\site-packages (from requests->transformers) (2022.12.7)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in c:\\users\\bhavi\\anaconda3\\envs\\myenv\\lib\\site-packages (from requests->transformers) (1.26.14)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\bhavi\\anaconda3\\envs\\myenv\\lib\\site-packages (from requests->transformers) (3.4)\n",
      "Requirement already satisfied: charset-normalizer<3,>=2 in c:\\users\\bhavi\\anaconda3\\envs\\myenv\\lib\\site-packages (from requests->transformers) (2.0.4)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install transformers\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from sklearn.model_selection import train_test_split\n",
    "from transformers import BertTokenizer, BertForSequenceClassification\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertForSequenceClassification: ['cls.seq_relationship.bias', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.dense.weight', 'cls.seq_relationship.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.bias']\n",
      "- This IS expected if you are initializing BertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BERT Model Metrics:\n",
      "Accuracy: 0.5527950310559007\n",
      "Precision: 0.5527950310559007\n",
      "Recall: 1.0\n",
      "F1 Score: 0.7120000000000001\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Load a pre-trained BERT tokenizer and model\n",
    "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
    "model = BertForSequenceClassification.from_pretrained('bert-base-uncased')\n",
    "\n",
    "# Tokenize and encode the text data\n",
    "X_train_encoded = tokenizer(X_fakes_train, padding=True, truncation=True, return_tensors='pt', max_length=128)\n",
    "X_test_encoded = tokenizer(X_fakes_test, padding=True, truncation=True, return_tensors='pt', max_length=128)\n",
    "\n",
    "# Convert labels to tensors\n",
    "y_train_tensor = torch.tensor(y_fakes_train)\n",
    "y_test_tensor = torch.tensor(y_fakes_test)\n",
    "\n",
    "# Forward pass through the model\n",
    "with torch.no_grad():\n",
    "    logits = model(**X_test_encoded).logits\n",
    "\n",
    "# Predictions\n",
    "y_pred = torch.argmax(logits, dim=1).numpy()\n",
    "\n",
    "# Calculate evaluation metrics\n",
    "accuracy_tr = accuracy_score(y_test_tensor.numpy(), y_pred)\n",
    "precision_tr = precision_score(y_test_tensor.numpy(), y_pred)\n",
    "recall_tr = recall_score(y_test_tensor.numpy(), y_pred)\n",
    "f1_tr = f1_score(y_test_tensor.numpy(), y_pred)\n",
    "\n",
    "print(\"BERT Model Metrics:\")\n",
    "print(\"Accuracy:\", accuracy_tr)\n",
    "print(\"Precision:\", precision_tr)\n",
    "print(\"Recall:\", recall_tr)\n",
    "print(\"F1 Score:\", f1_tr)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "CNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CNN-only Model Summary:\n",
      "Model: \"sequential_7\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " embedding_7 (Embedding)     (None, 300, 100)          93000     \n",
      "                                                                 \n",
      " conv1d_7 (Conv1D)           (None, 296, 128)          64128     \n",
      "                                                                 \n",
      " max_pooling1d_7 (MaxPooling  (None, 148, 128)         0         \n",
      " 1D)                                                             \n",
      "                                                                 \n",
      " flatten (Flatten)           (None, 18944)             0         \n",
      "                                                                 \n",
      " dense_7 (Dense)             (None, 64)                1212480   \n",
      "                                                                 \n",
      " dense_8 (Dense)             (None, 1)                 65        \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 1,369,673\n",
      "Trainable params: 1,276,673\n",
      "Non-trainable params: 93,000\n",
      "_________________________________________________________________\n",
      "Epoch 1/10\n",
      "9/9 [==============================] - 2s 83ms/step - loss: 0.7008 - accuracy: 0.4922 - val_loss: 0.7048 - val_accuracy: 0.5116\n",
      "Epoch 2/10\n",
      "9/9 [==============================] - 1s 58ms/step - loss: 0.6850 - accuracy: 0.5292 - val_loss: 0.7039 - val_accuracy: 0.5194\n",
      "Epoch 3/10\n",
      "9/9 [==============================] - 1s 64ms/step - loss: 0.6492 - accuracy: 0.6245 - val_loss: 0.6983 - val_accuracy: 0.5504\n",
      "Epoch 4/10\n",
      "9/9 [==============================] - 1s 71ms/step - loss: 0.6052 - accuracy: 0.7062 - val_loss: 0.7576 - val_accuracy: 0.4729\n",
      "Epoch 5/10\n",
      "9/9 [==============================] - 1s 72ms/step - loss: 0.5560 - accuracy: 0.7451 - val_loss: 0.7479 - val_accuracy: 0.5426\n",
      "Epoch 6/10\n",
      "9/9 [==============================] - 1s 73ms/step - loss: 0.5130 - accuracy: 0.7510 - val_loss: 0.7706 - val_accuracy: 0.5194\n",
      "Epoch 7/10\n",
      "9/9 [==============================] - 1s 70ms/step - loss: 0.4393 - accuracy: 0.8327 - val_loss: 0.8295 - val_accuracy: 0.5426\n",
      "Epoch 8/10\n",
      "9/9 [==============================] - 1s 74ms/step - loss: 0.4160 - accuracy: 0.8074 - val_loss: 0.8512 - val_accuracy: 0.4806\n",
      "Epoch 9/10\n",
      "9/9 [==============================] - 1s 77ms/step - loss: 0.3682 - accuracy: 0.8638 - val_loss: 0.8379 - val_accuracy: 0.5736\n",
      "Epoch 10/10\n",
      "9/9 [==============================] - 1s 96ms/step - loss: 0.3300 - accuracy: 0.9008 - val_loss: 0.8446 - val_accuracy: 0.5504\n",
      "6/6 [==============================] - 0s 10ms/step\n",
      "CNN-only Model Evaluation Results:\n",
      "Accuracy: 0.4906832298136646\n",
      "Precision: 0.5360824742268041\n",
      "Recall: 0.5842696629213483\n",
      "F1 Score: 0.5591397849462366\n"
     ]
    }
   ],
   "source": [
    "from keras.models import Sequential\n",
    "from keras.layers import Embedding, Conv1D, MaxPooling1D, Flatten, Dense\n",
    "\n",
    "# Model Architecture for CNN-only\n",
    "def create_cnn_model(embedding_matrix, max_sequence_length, embedding_dim):\n",
    "    model = Sequential()\n",
    "    model.add(Embedding(embedding_matrix.shape[0], embedding_dim, weights=[embedding_matrix], input_length=max_sequence_length, trainable=False))\n",
    "    model.add(Conv1D(filters=128, kernel_size=5, activation='relu'))\n",
    "    model.add(MaxPooling1D(pool_size=2))\n",
    "    model.add(Flatten())\n",
    "    model.add(Dense(64, activation='relu'))\n",
    "    model.add(Dense(1, activation='sigmoid'))\n",
    "\n",
    "    # Compile\n",
    "    model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])\n",
    "    return model\n",
    "\n",
    "# CNN-only model for FA-KES dataset\n",
    "cnn_model = create_cnn_model(embedding_matrix_fakes, max_sequence_length, embedding_dim)\n",
    "\n",
    "# Summary of CNN-only Model Architecture for FA-KES dataset\n",
    "print(\"CNN-only Model Summary:\")\n",
    "cnn_model.summary()\n",
    "\n",
    "# Train the CNN-only model\n",
    "history_cnn = cnn_model.fit(X_fakes_train_padded, y_fakes_train, epochs=10, batch_size=64, validation_split=0.2, verbose=1)\n",
    "\n",
    "# Evaluate the CNN-only model on the test set\n",
    "y_cnn_pred = cnn_model.predict(X_fakes_test_padded)\n",
    "y_cnn_pred = (y_cnn_pred > 0.5)  # Convert probabilities to binary predictions\n",
    "\n",
    "# Calculate evaluation metrics\n",
    "accuracy_cnn = accuracy_score(y_fakes_test, y_cnn_pred)\n",
    "precision_cnn = precision_score(y_fakes_test, y_cnn_pred)\n",
    "recall_cnn = recall_score(y_fakes_test, y_cnn_pred)\n",
    "f1_score_cnn = f1_score(y_fakes_test, y_cnn_pred)\n",
    "\n",
    "# Print the evaluation results\n",
    "print(\"CNN-only Model Evaluation Results:\")\n",
    "print(\"Accuracy:\", accuracy_cnn)\n",
    "print(\"Precision:\", precision_cnn)\n",
    "print(\"Recall:\", recall_cnn)\n",
    "print(\"F1 Score:\", f1_score_cnn)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "RNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RNN-only Model Summary:\n",
      "Model: \"sequential_8\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " embedding_8 (Embedding)     (None, 300, 100)          93000     \n",
      "                                                                 \n",
      " lstm_7 (LSTM)               (None, 32)                17024     \n",
      "                                                                 \n",
      " dense_9 (Dense)             (None, 1)                 33        \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 110,057\n",
      "Trainable params: 17,057\n",
      "Non-trainable params: 93,000\n",
      "_________________________________________________________________\n",
      "Epoch 1/10\n",
      "9/9 [==============================] - 4s 189ms/step - loss: 0.6974 - accuracy: 0.5175 - val_loss: 0.6978 - val_accuracy: 0.5194\n",
      "Epoch 2/10\n",
      "9/9 [==============================] - 1s 130ms/step - loss: 0.6847 - accuracy: 0.5350 - val_loss: 0.6938 - val_accuracy: 0.5581\n",
      "Epoch 3/10\n",
      "9/9 [==============================] - 1s 147ms/step - loss: 0.6748 - accuracy: 0.6148 - val_loss: 0.6852 - val_accuracy: 0.5891\n",
      "Epoch 4/10\n",
      "9/9 [==============================] - 1s 145ms/step - loss: 0.6741 - accuracy: 0.5837 - val_loss: 0.6917 - val_accuracy: 0.5581\n",
      "Epoch 5/10\n",
      "9/9 [==============================] - 1s 135ms/step - loss: 0.6759 - accuracy: 0.5409 - val_loss: 0.6951 - val_accuracy: 0.5581\n",
      "Epoch 6/10\n",
      "9/9 [==============================] - 1s 155ms/step - loss: 0.6682 - accuracy: 0.5817 - val_loss: 0.6839 - val_accuracy: 0.5969\n",
      "Epoch 7/10\n",
      "9/9 [==============================] - 1s 148ms/step - loss: 0.6638 - accuracy: 0.6362 - val_loss: 0.6831 - val_accuracy: 0.5814\n",
      "Epoch 8/10\n",
      "9/9 [==============================] - 1s 142ms/step - loss: 0.6621 - accuracy: 0.6226 - val_loss: 0.6848 - val_accuracy: 0.5581\n",
      "Epoch 9/10\n",
      "9/9 [==============================] - 1s 127ms/step - loss: 0.6574 - accuracy: 0.6323 - val_loss: 0.6873 - val_accuracy: 0.5736\n",
      "Epoch 10/10\n",
      "9/9 [==============================] - 1s 141ms/step - loss: 0.6543 - accuracy: 0.6362 - val_loss: 0.6855 - val_accuracy: 0.6124\n",
      "6/6 [==============================] - 0s 24ms/step\n",
      "RNN-only Model Evaluation Results:\n",
      "Accuracy: 0.484472049689441\n",
      "Precision: 0.5319148936170213\n",
      "Recall: 0.5617977528089888\n",
      "F1 Score: 0.5464480874316939\n"
     ]
    }
   ],
   "source": [
    "from keras.models import Sequential\n",
    "from keras.layers import Embedding, LSTM, Dense\n",
    "\n",
    "# Model Architecture for RNN-only\n",
    "def create_rnn_model(embedding_matrix, max_sequence_length, embedding_dim):\n",
    "    model = Sequential()\n",
    "    model.add(Embedding(embedding_matrix.shape[0], embedding_dim, weights=[embedding_matrix], input_length=max_sequence_length, trainable=False))\n",
    "    model.add(LSTM(32))\n",
    "    model.add(Dense(1, activation='sigmoid'))\n",
    "\n",
    "    # Compile the model\n",
    "    model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])\n",
    "    return model\n",
    "\n",
    "# Create the RNN-only model for FA-KES dataset\n",
    "rnn_model = create_rnn_model(embedding_matrix_fakes, max_sequence_length, embedding_dim)\n",
    "\n",
    "# Summary of RNN-only Model Architecture for FA-KES dataset\n",
    "print(\"RNN-only Model Summary:\")\n",
    "rnn_model.summary()\n",
    "\n",
    "# Train the RNN-only model\n",
    "history_rnn = rnn_model.fit(X_fakes_train_padded, y_fakes_train, epochs=10, batch_size=64, validation_split=0.2, verbose=1)\n",
    "\n",
    "# Evaluate the RNN-only model on the test set\n",
    "y_rnn_pred = rnn_model.predict(X_fakes_test_padded)\n",
    "y_rnn_pred = (y_rnn_pred > 0.5)  # Convert probabilities to binary predictions\n",
    "\n",
    "# Calculate evaluation metrics\n",
    "accuracy_rnn = accuracy_score(y_fakes_test, y_rnn_pred)\n",
    "precision_rnn = precision_score(y_fakes_test, y_rnn_pred)\n",
    "recall_rnn = recall_score(y_fakes_test, y_rnn_pred)\n",
    "f1_score_rnn = f1_score(y_fakes_test, y_rnn_pred)\n",
    "\n",
    "# Print the evaluation results\n",
    "print(\"RNN-only Model Evaluation Results:\")\n",
    "print(\"Accuracy:\", accuracy_rnn)\n",
    "print(\"Precision:\", precision_rnn)\n",
    "print(\"Recall:\", recall_rnn)\n",
    "print(\"F1 Score:\", f1_score_rnn)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "BiLSTM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "9/9 [==============================] - 5s 212ms/step - loss: 0.6998 - accuracy: 0.5000 - val_loss: 0.6958 - val_accuracy: 0.5349\n",
      "Epoch 2/10\n",
      "9/9 [==============================] - 1s 131ms/step - loss: 0.6920 - accuracy: 0.5311 - val_loss: 0.6971 - val_accuracy: 0.4961\n",
      "Epoch 3/10\n",
      "9/9 [==============================] - 1s 140ms/step - loss: 0.6877 - accuracy: 0.5447 - val_loss: 0.6912 - val_accuracy: 0.5426\n",
      "Epoch 4/10\n",
      "9/9 [==============================] - 1s 135ms/step - loss: 0.6864 - accuracy: 0.5545 - val_loss: 0.6901 - val_accuracy: 0.5349\n",
      "Epoch 5/10\n",
      "9/9 [==============================] - 1s 133ms/step - loss: 0.6840 - accuracy: 0.5798 - val_loss: 0.6931 - val_accuracy: 0.5349\n",
      "Epoch 6/10\n",
      "9/9 [==============================] - 1s 133ms/step - loss: 0.6849 - accuracy: 0.5428 - val_loss: 0.6986 - val_accuracy: 0.4961\n",
      "Epoch 7/10\n",
      "9/9 [==============================] - 1s 135ms/step - loss: 0.6829 - accuracy: 0.5409 - val_loss: 0.6929 - val_accuracy: 0.5349\n",
      "Epoch 8/10\n",
      "9/9 [==============================] - 1s 133ms/step - loss: 0.6789 - accuracy: 0.5720 - val_loss: 0.6884 - val_accuracy: 0.5504\n",
      "Epoch 9/10\n",
      "9/9 [==============================] - 1s 142ms/step - loss: 0.6786 - accuracy: 0.6031 - val_loss: 0.6887 - val_accuracy: 0.5349\n",
      "Epoch 10/10\n",
      "9/9 [==============================] - 1s 133ms/step - loss: 0.6739 - accuracy: 0.5992 - val_loss: 0.6877 - val_accuracy: 0.5659\n",
      "6/6 [==============================] - 1s 32ms/step\n",
      "Bidirectional LSTM Model Metrics:\n",
      "Accuracy: 0.5279503105590062\n",
      "Precision: 0.5503875968992248\n",
      "Recall: 0.797752808988764\n",
      "F1 Score: 0.6513761467889908\n"
     ]
    }
   ],
   "source": [
    "# Create the model for FA-KES dataset with only Bidirectional LSTM\n",
    "def create_bilstm_model(embedding_matrix, max_sequence_length, embedding_dim):\n",
    "    model = Sequential()\n",
    "    model.add(Embedding(embedding_matrix.shape[0], embedding_dim, weights=[embedding_matrix], input_length=max_sequence_length, trainable=False))\n",
    "    model.add(Bidirectional(LSTM(32)))  # Use Bidirectional LSTM\n",
    "    model.add(Dense(1, activation='sigmoid'))\n",
    "\n",
    "    # Compile the model\n",
    "    model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])\n",
    "    return model\n",
    "\n",
    "# Create and train the Bidirectional LSTM model\n",
    "model_bilstm_fakes = create_bilstm_model(embedding_matrix_fakes, max_sequence_length, embedding_dim)\n",
    "history_bilstm_fakes = model_bilstm_fakes.fit(X_fakes_train_padded, y_fakes_train, epochs=10, batch_size=64, validation_split=0.2, verbose=1)\n",
    "\n",
    "# Evaluate the Bidirectional LSTM model on the test set\n",
    "y_fakes_bilstm_pred_probs = model_bilstm_fakes.predict(X_fakes_test_padded)\n",
    "y_fakes_bilstm_pred = (y_fakes_bilstm_pred_probs > 0.5).astype(int)\n",
    "\n",
    "# Calculate evaluation metrics\n",
    "accuracy_bilstm = accuracy_score(y_fakes_test, y_fakes_bilstm_pred)\n",
    "precision_bilstm = precision_score(y_fakes_test, y_fakes_bilstm_pred)\n",
    "recall_bilstm = recall_score(y_fakes_test, y_fakes_bilstm_pred)\n",
    "f1_bilstm = f1_score(y_fakes_test, y_fakes_bilstm_pred)\n",
    "\n",
    "print(\"Bidirectional LSTM Model Metrics:\")\n",
    "print(\"Accuracy:\", accuracy_bilstm)\n",
    "print(\"Precision:\", precision_bilstm)\n",
    "print(\"Recall:\", recall_bilstm)\n",
    "print(\"F1 Score:\", f1_bilstm)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Classifiers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\bhavi\\anaconda3\\envs\\myenv\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:818: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG,\n"
     ]
    }
   ],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.ensemble import RandomForestClassifier, AdaBoostClassifier\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\n",
    "import numpy as np\n",
    "from sklearn.linear_model import SGDClassifier\n",
    "# Define classifiers\n",
    "classifiers = {\n",
    "    \"LR\": LogisticRegression(),\n",
    "    \"RF\": RandomForestClassifier(),\n",
    "    \"MNB\": MultinomialNB(),\n",
    "    \"SGD\": SGDClassifier(),\n",
    "    \"KNNs\": KNeighborsClassifier(),\n",
    "    \"DT\": DecisionTreeClassifier(),\n",
    "    \"AB\": AdaBoostClassifier(),\n",
    "}\n",
    "# Train and evaluate classifiers\n",
    "results = {}\n",
    "for clf_name, clf in classifiers.items():\n",
    "    clf.fit(X_fakes_train_padded, y_fakes_train)\n",
    "    y_pred = clf.predict(X_fakes_test_padded)\n",
    "\n",
    "    accuracy = accuracy_score(y_fakes_test, y_pred)\n",
    "    precision = precision_score(y_fakes_test, y_pred)\n",
    "    recall = recall_score(y_fakes_test, y_pred)\n",
    "    f1 = f1_score(y_fakes_test, y_pred)\n",
    "\n",
    "    results[clf_name] = {\n",
    "        \"Accuracy\": accuracy,\n",
    "        \"Precision\": precision,\n",
    "        \"Recall\": recall,\n",
    "        \"F1 Score\": f1,\n",
    "    }\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "hybrid_results = {\n",
    "    \"Hybrid CNN-RNN(LSTM)\": {\n",
    "        \"Accuracy\": accuracy_fakes,\n",
    "        \"Precision\": precision_fakes,\n",
    "        \"Recall\": recall_fakes,\n",
    "        \"F1 Score\": f1_score_fakes,\n",
    "    }\n",
    "}\n",
    "hybrid_bilstmresults={\n",
    "    \"Hybrid CNN-RNN(BiLSTM)\":{\n",
    "        \"Accuracy\": accuracy_hcnnrnn,\n",
    "        \"Precision\": precision_hcnnrnn,\n",
    "        \"Recall\": recall_hcnnrnn,\n",
    "        \"F1 Score\": f1_hcnnrnn,\n",
    "    }\n",
    "}\n",
    "transformer_results={\n",
    "    \"Transformers\":{\n",
    "        \"Accuracy\": accuracy_tr,\n",
    "        \"Precision\": precision_tr,\n",
    "        \"Recall\": recall_tr,\n",
    "        \"F1 Score\": f1_tr,\n",
    "    }\n",
    "}\n",
    "BilstmOnly_results={\n",
    "    \"BiLSTMOnly\":{\n",
    "        \"Accuracy\": accuracy_bilstm,\n",
    "        \"Precision\": precision_bilstm,\n",
    "        \"Recall\": recall_bilstm,\n",
    "        \"F1 Score\": f1_bilstm,\n",
    "    }\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Results of all models on the FA-KES dataset:\n",
      "Classifier\tAccuracy\tPrecision\tRecall\t\tF1 Score\n",
      "LR\t\t0.50\t\t0.54\t\t0.65\t\t0.59\n",
      "RF\t\t0.45\t\t0.51\t\t0.52\t\t0.51\n",
      "MNB\t\t0.44\t\t0.49\t\t0.36\t\t0.42\n",
      "SGD\t\t0.56\t\t0.56\t\t0.99\t\t0.71\n",
      "KNNs\t\t0.50\t\t0.54\t\t0.56\t\t0.55\n",
      "DT\t\t0.53\t\t0.58\t\t0.53\t\t0.55\n",
      "AB\t\t0.45\t\t0.51\t\t0.44\t\t0.47\n",
      "Hybrid CNN-RNN(LSTM)\t\t0.52\t\t0.55\t\t0.65\t\t0.60\n",
      "Hybrid CNN-RNN(BiLSTM)\t\t0.45\t\t0.51\t\t0.46\t\t0.48\n",
      "Transformers\t\t0.55\t\t0.55\t\t1.00\t\t0.71\n",
      "BiLSTMOnly\t\t0.53\t\t0.55\t\t0.80\t\t0.65\n",
      "CNN-only\t\t0.49\t\t0.54\t\t0.58\t\t0.56\n",
      "RNN-only\t\t0.48\t\t0.53\t\t0.56\t\t0.55\n"
     ]
    }
   ],
   "source": [
    "# Combine the results of all models\n",
    "all_results = {\n",
    "    **results,\n",
    "    **hybrid_results,\n",
    "    **hybrid_bilstmresults,\n",
    "    **transformer_results,\n",
    "    **BilstmOnly_results,\n",
    "    'CNN-only': {\n",
    "        'Accuracy': accuracy_cnn,\n",
    "        'Precision': precision_cnn,\n",
    "        'Recall': recall_cnn,\n",
    "        'F1 Score': f1_score_cnn\n",
    "    },\n",
    "    'RNN-only': {\n",
    "        'Accuracy': accuracy_rnn,\n",
    "        'Precision': precision_rnn,\n",
    "        'Recall': recall_rnn,\n",
    "        'F1 Score': f1_score_rnn\n",
    "    }\n",
    "\n",
    "}\n",
    "\n",
    "# Print the comparison table\n",
    "print(\"Results of all models on the FA-KES dataset:\")\n",
    "print(\"Classifier\\tAccuracy\\tPrecision\\tRecall\\t\\tF1 Score\")\n",
    "for clf_name, metrics in all_results.items():\n",
    "    print(f\"{clf_name}\\t\\t{metrics['Accuracy']:.2f}\\t\\t{metrics['Precision']:.2f}\\t\\t{metrics['Recall']:.2f}\\t\\t{metrics['F1 Score']:.2f}\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "myenv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
